{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-28T19:05:53.260841Z",
     "start_time": "2025-08-28T19:05:34.292103Z"
    }
   },
   "id": "b1453ffc69f67267"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Step 1: combine job_posting_details and job_description files"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7911efeb6bb46dc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jpd1 = pd.read_csv('/Users/apoltavets/anna-apps/annalyticai/careear-stats/output/job_posting_details/20250825-2005.tsv', sep='\\t')\n",
    "jpd2 = pd.read_csv('/Users/apoltavets/anna-apps/annalyticai/careear-stats/output/job_posting_details/20250825-2056.tsv', sep='\\t')\n",
    "jpd3 = pd.read_csv('/Users/apoltavets/anna-apps/annalyticai/careear-stats/output/job_posting_details/20250825-2059.tsv', sep='\\t')\n",
    "\n",
    "jpd = pd.concat([jpd1, jpd2, jpd3], ignore_index=True)\n",
    "jpd.drop_duplicates(subset=['linkedin_job_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "jd1 = pd.read_csv('/Users/apoltavets/anna-apps/annalyticai/careear-stats/output/job_description/20250825-2005.tsv', sep='\\t')\n",
    "jd2 = pd.read_csv('/Users/apoltavets/anna-apps/annalyticai/careear-stats/output/job_description/20250825-2056.tsv', sep='\\t')\n",
    "jd3 = pd.read_csv('/Users/apoltavets/anna-apps/annalyticai/careear-stats/output/job_description/20250825-2059.tsv', sep='\\t')\n",
    "\n",
    "jd = pd.concat([jd1, jd2, jd3], ignore_index=True)\n",
    "jd.drop_duplicates(subset=['linkedin_job_id'], inplace=True)\n",
    "jd = jd.drop(['job_title'], axis=1)\n",
    "df = jpd.merge(jd, on='linkedin_job_id', how='inner')\n",
    "df.to_csv('/Users/apoltavets/anna-apps/annalyticai/careear-stats/output/linkedin_jobs_20250825.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e46e7ad6028417ad"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Step 2: clean job titles"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1fa138e76faae0c9"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# Load the combined data\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m df = \u001B[43mpd\u001B[49m.read_csv(\u001B[33m'\u001B[39m\u001B[33m/Users/apoltavets/anna-apps/annalyticai/careear-stats/output/linkedin_jobs_20250825.csv\u001B[39m\u001B[33m'\u001B[39m)\n",
      "\u001B[31mNameError\u001B[39m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the combined data\n",
    "df = pd.read_csv('/Users/apoltavets/anna-apps/annalyticai/careear-stats/output/linkedin_jobs_20250825.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-28T19:04:45.625658Z",
     "start_time": "2025-08-28T19:04:45.336914Z"
    }
   },
   "id": "ff9ce2b6003934d"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def title_cleanup(job_title):\n",
    "       if not isinstance(job_title, str):  # Check if the value is not a string\n",
    "              return job_title\n",
    "\n",
    "       tmp = re.sub(r'\\(.*?\\)', '', job_title).strip()\n",
    "       tmp1 = re.sub(r'-\\s*\\d+[,\\d]*/\\w+\\s+\\w+$', '', tmp).strip()\n",
    "       tmp2 = tmp1.replace('Sr.', 'Senior').replace('Sr ', 'Senior ')\n",
    "       return tmp2\n",
    "\n",
    "# Apply the function\n",
    "df['job_title_upd'] = df['job_title'].apply(title_cleanup)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-27T05:50:18.183952Z",
     "start_time": "2025-08-27T05:50:18.148530Z"
    }
   },
   "id": "9cb036599ee04c2d"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2d178c93e810dc9a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Step 3: filter out healthcare, education, finance, legal, construction, customer service jobs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "671de834c588474b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import re\n",
    "titles = df[\"job_title\"].drop_duplicates().to_list()\n",
    "titles1 = []\n",
    "for t in titles:\n",
    "    titles1.append(re.sub(r'[^a-zA-Z\\s]', '', str(t)))\n",
    "    \n",
    "keywords = (\" \".join(titles1)).lower().split(\" \")\n",
    "kdf = pd.DataFrame(keywords, columns=['keyword'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2db50f403d5c7cc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "kdf.groupby('keyword').size().sort_values(ascending=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dae39e638ab7ab28"
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "outputs": [],
   "source": [
    "df_filtered =(df[~df['organization_id'].isin([9185, 55381, 10914, 24034, 6755,\n",
    "       # health organizations\n",
    "       25522,\n",
    "       10769,\n",
    "       30412,\n",
    "       14461,\n",
    "       1340385,\n",
    "       27573,\n",
    "       33458365,\n",
    "       1016,\n",
    "       292629,\n",
    "       11580,\n",
    "       6250,\n",
    "       265869,\n",
    "       38521,\n",
    "       42925,\n",
    "       854537,\n",
    "       166209,\n",
    "       11819824,\n",
    "       166821,\n",
    "       8302,\n",
    "       104175816\n",
    "       ])]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-26T05:50:41.906167Z",
     "start_time": "2025-08-26T05:50:41.894394Z"
    }
   },
   "id": "30071986cdc32a18"
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 11356 entries, 0 to 12061\n",
      "Data columns (total 9 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   linkedin_job_id     11356 non-null  int64  \n",
      " 1   organization_name   11355 non-null  object \n",
      " 2   location            11356 non-null  object \n",
      " 3   job_title           11352 non-null  object \n",
      " 4   payer_name          11355 non-null  object \n",
      " 5   organization_id     11356 non-null  int64  \n",
      " 6   job_post_date       11356 non-null  object \n",
      " 7   job_posting_millis  11356 non-null  float64\n",
      " 8   job_description     11356 non-null  object \n",
      "dtypes: float64(1), int64(2), object(6)\n",
      "memory usage: 887.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df_filtered.info()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-26T05:50:42.600786Z",
     "start_time": "2025-08-26T05:50:42.592917Z"
    }
   },
   "id": "a45feee2a780102d"
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x8/ps_xddr95734lgrb6gqpr7g80000gp/T/ipykernel_60544/918165978.py:32: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  df_filtered = df_filtered[conditions]\n"
     ]
    }
   ],
   "source": [
    "conditions = (~(df['job_title'].str.lower().str.contains('nurse', na=False)) &\n",
    "              (~df['job_title'].str.lower().str.contains('patient', na=False)) &\n",
    "              (~df['job_title'].str.lower().str.contains('clinic', na=False)) &\n",
    "              (~df['job_title'].str.lower().str.contains('therapist', na=False)) &\n",
    "              (~df['job_title'].str.lower().str.contains('radiologic', na=False)) &\n",
    "              (~df['job_title'].str.lower().str.contains('plebotomist', na=False)) &\n",
    "              (~df['job_title'].str.lower().str.contains('veterinarian', na=False)) &\n",
    "              (~df['job_title'].str.lower().str.contains('perfusionist', na=False)) &\n",
    "              (~df['job_title'].str.lower().str.contains('ophtalmic', na=False)) &\n",
    "              (~df['job_title'].str.lower().str.contains('surgery', na=False)) &\n",
    "              (~df['job_title'].str.lower().str.contains('cardiology', na=False)) &\n",
    "              (~df['job_title'].str.lower().str.contains('surgen', na=False)) &\n",
    "              (~df['job_title'].str.lower().str.contains('hospital ', na=False)) &\n",
    "              (~df['job_title'].str.lower().str.contains('hospital,', na=False)) &\n",
    "              (~df['job_title'].str.lower().str.contains('controller', na=False)) &\n",
    "              (~df['job_title'].str.lower().str.contains('tax', na=False)) &\n",
    "              (~df['job_title'].str.lower().str.contains('accountant', na=False)) &\n",
    "              (~df['job_title'].str.lower().str.contains('financial consultant', na=False)) &\n",
    "              (~df['job_title'].str.lower().str.contains('administrative assistant', na=False)) &\n",
    "              (~df['job_title'].str.lower().str.contains('teacher', na=False)) &\n",
    "              (~df['job_title'].str.lower().str.contains('coordinator', na=False)) &\n",
    "              (~df['job_title'].str.lower().str.contains('superintendent', na=False)) &\n",
    "              (~df['job_title'].str.lower().str.contains('bridge', na=False)) &\n",
    "              (~df['job_title'].str.lower().str.contains('estimator', na=False)) &\n",
    "              (~df['job_title'].str.lower().str.contains('legal', na=False)) &\n",
    "              (~df['job_title'].str.lower().str.contains('call center', na=False)) &\n",
    "              (~df['job_title'].str.lower().str.contains('representative', na=False)) &\n",
    "              (~df['job_title'].str.lower().str.contains('civil', na=False)) &\n",
    "              (~df['job_title'].str.lower().str.contains('baker', na=False))\n",
    "              )\n",
    "\n",
    "df_filtered = df_filtered[conditions]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-26T05:50:49.651915Z",
     "start_time": "2025-08-26T05:50:49.417595Z"
    }
   },
   "id": "826094f55a56875d"
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "outputs": [],
   "source": [
    "dfu = (df_filtered[['linkedin_job_id', 'job_title', 'organization_name', 'job_description', 'payer_name', 'organization_id']].groupby(['job_title', 'organization_name', 'job_description', 'payer_name', 'organization_id'])\n",
    " .first()\n",
    ".reset_index()\n",
    ".sort_values(by=['job_title', 'organization_name']))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-26T05:53:43.754709Z",
     "start_time": "2025-08-26T05:53:43.621679Z"
    }
   },
   "id": "e9dd885274d13721"
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [
    {
     "data": {
      "text/plain": "np.int64(5834)"
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfu.count().max()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-26T05:53:45.665617Z",
     "start_time": "2025-08-26T05:53:45.657793Z"
    }
   },
   "id": "9edf54375bc55fea"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lensa is a career site that helps job seekers find great jobs in the US. We are not a staffing firm or agency.\n",
    " Lensa does not hire directly for these jobs, but promotes jobs on LinkedIn on behalf ..."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ab24201224bc6d7"
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "dfu = pd.read_csv('/Users/apoltavets/anna-apps/annalyticai/careear-stats/output/dfu_20250825_filtered.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-27T05:59:57.273836Z",
     "start_time": "2025-08-27T05:59:56.702754Z"
    }
   },
   "id": "8b93dda184be953f"
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "dfu['is_it'] = 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-27T05:59:57.291622Z",
     "start_time": "2025-08-27T05:59:57.274372Z"
    }
   },
   "id": "c68920986f73c1e"
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "df_dfu = df[['job_title', 'organization_name']].merge(dfu[['job_title', 'organization_name', 'is_it']], on=['job_title', 'organization_name'], how='left')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-27T06:00:20.281013Z",
     "start_time": "2025-08-27T06:00:20.259305Z"
    }
   },
   "id": "a2b136cfb5de75e"
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "df_dfu['is_it'] = df_dfu['is_it'].fillna(0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-27T06:00:41.870267Z",
     "start_time": "2025-08-27T06:00:41.861305Z"
    }
   },
   "id": "30deb835a1f1de26"
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "data": {
      "text/plain": "is_it\n1.0    16630\n0.0     2179\nName: count, dtype: int64"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dfu['is_it'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-27T06:08:39.499833Z",
     "start_time": "2025-08-27T06:08:39.495090Z"
    }
   },
   "id": "19bf9c50547913b1"
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "df_dfu.to_csv('/Users/apoltavets/anna-apps/annalyticai/careear-stats/output/df_dfu_20250825_filtered.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-27T06:11:45.651236Z",
     "start_time": "2025-08-27T06:11:45.587070Z"
    }
   },
   "id": "39fb8c4f76b5d25b"
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "\n",
    "def predict_it(job_title):\n",
    "    X = [job_title]\n",
    "    MODEL_PATH = \"models/it_classifier_rf.joblib\"\n",
    "    ENCODER_PATH = \"models/e5_encoder.joblib\"\n",
    "    \n",
    "    # Use joblib to load the model and encoder\n",
    "    clf = load(MODEL_PATH)\n",
    "    enc = load(ENCODER_PATH)\n",
    "    \n",
    "    # Encode the input and make predictions\n",
    "    X_encoded = enc.encode(X)\n",
    "    y = clf.predict(X_encoded)\n",
    "    return y[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-27T06:15:49.190253Z",
     "start_time": "2025-08-27T06:15:49.167566Z"
    }
   },
   "id": "cca111c517a8bf1c"
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[78]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m df_dfu[\u001B[33m'\u001B[39m\u001B[33mpredicted\u001B[39m\u001B[33m'\u001B[39m] = \u001B[43mpredict_it\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf_dfu\u001B[49m\u001B[43m.\u001B[49m\u001B[43mloc\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mjob_title\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[76]\u001B[39m\u001B[32m, line 10\u001B[39m, in \u001B[36mpredict_it\u001B[39m\u001B[34m(job_title)\u001B[39m\n\u001B[32m      8\u001B[39m \u001B[38;5;66;03m# Use joblib to load the model and encoder\u001B[39;00m\n\u001B[32m      9\u001B[39m clf = load(MODEL_PATH)\n\u001B[32m---> \u001B[39m\u001B[32m10\u001B[39m enc = \u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mENCODER_PATH\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     12\u001B[39m \u001B[38;5;66;03m# Encode the input and make predictions\u001B[39;00m\n\u001B[32m     13\u001B[39m X_encoded = enc.encode(X)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/annalyticai/lib/python3.11/site-packages/joblib/numpy_pickle.py:749\u001B[39m, in \u001B[36mload\u001B[39m\u001B[34m(filename, mmap_mode, ensure_native_byte_order)\u001B[39m\n\u001B[32m    744\u001B[39m                 \u001B[38;5;28;01mreturn\u001B[39;00m load_compatibility(fobj)\n\u001B[32m    746\u001B[39m             \u001B[38;5;66;03m# A memory-mapped array has to be mapped with the endianness\u001B[39;00m\n\u001B[32m    747\u001B[39m             \u001B[38;5;66;03m# it has been written with. Other arrays are coerced to the\u001B[39;00m\n\u001B[32m    748\u001B[39m             \u001B[38;5;66;03m# native endianness of the host system.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m749\u001B[39m             obj = \u001B[43m_unpickle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    750\u001B[39m \u001B[43m                \u001B[49m\u001B[43mfobj\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    751\u001B[39m \u001B[43m                \u001B[49m\u001B[43mensure_native_byte_order\u001B[49m\u001B[43m=\u001B[49m\u001B[43mensure_native_byte_order\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    752\u001B[39m \u001B[43m                \u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    753\u001B[39m \u001B[43m                \u001B[49m\u001B[43mmmap_mode\u001B[49m\u001B[43m=\u001B[49m\u001B[43mvalidated_mmap_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    754\u001B[39m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    756\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m obj\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/annalyticai/lib/python3.11/site-packages/joblib/numpy_pickle.py:626\u001B[39m, in \u001B[36m_unpickle\u001B[39m\u001B[34m(fobj, ensure_native_byte_order, filename, mmap_mode)\u001B[39m\n\u001B[32m    624\u001B[39m obj = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    625\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m626\u001B[39m     obj = \u001B[43munpickler\u001B[49m\u001B[43m.\u001B[49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    627\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m unpickler.compat_mode:\n\u001B[32m    628\u001B[39m         warnings.warn(\n\u001B[32m    629\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mThe file \u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m has been generated with a \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    630\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mjoblib version less than 0.10. \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   (...)\u001B[39m\u001B[32m    633\u001B[39m             stacklevel=\u001B[32m3\u001B[39m,\n\u001B[32m    634\u001B[39m         )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/pickle.py:1213\u001B[39m, in \u001B[36m_Unpickler.load\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1211\u001B[39m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEOFError\u001B[39;00m\n\u001B[32m   1212\u001B[39m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key, bytes_types)\n\u001B[32m-> \u001B[39m\u001B[32m1213\u001B[39m         \u001B[43mdispatch\u001B[49m\u001B[43m[\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m   1214\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m _Stop \u001B[38;5;28;01mas\u001B[39;00m stopinst:\n\u001B[32m   1215\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m stopinst.value\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/pickle.py:1590\u001B[39m, in \u001B[36m_Unpickler.load_reduce\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1588\u001B[39m args = stack.pop()\n\u001B[32m   1589\u001B[39m func = stack[-\u001B[32m1\u001B[39m]\n\u001B[32m-> \u001B[39m\u001B[32m1590\u001B[39m stack[-\u001B[32m1\u001B[39m] = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/annalyticai/lib/python3.11/site-packages/torch/storage.py:371\u001B[39m, in \u001B[36m_load_from_bytes\u001B[39m\u001B[34m(b)\u001B[39m\n\u001B[32m    370\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_load_from_bytes\u001B[39m(b):\n\u001B[32m--> \u001B[39m\u001B[32m371\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mio\u001B[49m\u001B[43m.\u001B[49m\u001B[43mBytesIO\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/annalyticai/lib/python3.11/site-packages/torch/serialization.py:1040\u001B[39m, in \u001B[36mload\u001B[39m\u001B[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001B[39m\n\u001B[32m   1038\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m   1039\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m pickle.UnpicklingError(UNSAFE_MESSAGE + \u001B[38;5;28mstr\u001B[39m(e)) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1040\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_legacy_load\u001B[49m\u001B[43m(\u001B[49m\u001B[43mopened_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpickle_module\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mpickle_load_args\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/annalyticai/lib/python3.11/site-packages/torch/serialization.py:1268\u001B[39m, in \u001B[36m_legacy_load\u001B[39m\u001B[34m(f, map_location, pickle_module, **pickle_load_args)\u001B[39m\n\u001B[32m   1266\u001B[39m unpickler = UnpicklerWrapper(f, **pickle_load_args)\n\u001B[32m   1267\u001B[39m unpickler.persistent_load = persistent_load\n\u001B[32m-> \u001B[39m\u001B[32m1268\u001B[39m result = \u001B[43munpickler\u001B[49m\u001B[43m.\u001B[49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1270\u001B[39m deserialized_storage_keys = pickle_module.load(f, **pickle_load_args)\n\u001B[32m   1272\u001B[39m offset = f.tell() \u001B[38;5;28;01mif\u001B[39;00m f_should_read_directly \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/annalyticai/lib/python3.11/site-packages/torch/serialization.py:1205\u001B[39m, in \u001B[36m_legacy_load.<locals>.persistent_load\u001B[39m\u001B[34m(saved_id)\u001B[39m\n\u001B[32m   1201\u001B[39m     obj._torch_load_uninitialized = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m   1202\u001B[39m     \u001B[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001B[39;00m\n\u001B[32m   1203\u001B[39m     \u001B[38;5;66;03m# stop wrapping with TypedStorage\u001B[39;00m\n\u001B[32m   1204\u001B[39m     typed_storage = torch.storage.TypedStorage(\n\u001B[32m-> \u001B[39m\u001B[32m1205\u001B[39m         wrap_storage=\u001B[43mrestore_location\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlocation\u001B[49m\u001B[43m)\u001B[49m,\n\u001B[32m   1206\u001B[39m         dtype=dtype,\n\u001B[32m   1207\u001B[39m         _internal=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m   1208\u001B[39m     deserialized_objects[root_key] = typed_storage\n\u001B[32m   1209\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/annalyticai/lib/python3.11/site-packages/torch/serialization.py:391\u001B[39m, in \u001B[36mdefault_restore_location\u001B[39m\u001B[34m(storage, location)\u001B[39m\n\u001B[32m    389\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdefault_restore_location\u001B[39m(storage, location):\n\u001B[32m    390\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m _, _, fn \u001B[38;5;129;01min\u001B[39;00m _package_registry:\n\u001B[32m--> \u001B[39m\u001B[32m391\u001B[39m         result = \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstorage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlocation\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    392\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    393\u001B[39m             \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/annalyticai/lib/python3.11/site-packages/torch/serialization.py:266\u001B[39m, in \u001B[36m_cuda_deserialize\u001B[39m\u001B[34m(obj, location)\u001B[39m\n\u001B[32m    264\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_cuda_deserialize\u001B[39m(obj, location):\n\u001B[32m    265\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m location.startswith(\u001B[33m'\u001B[39m\u001B[33mcuda\u001B[39m\u001B[33m'\u001B[39m):\n\u001B[32m--> \u001B[39m\u001B[32m266\u001B[39m         device = \u001B[43mvalidate_cuda_device\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlocation\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    267\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(obj, \u001B[33m\"\u001B[39m\u001B[33m_torch_load_uninitialized\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[32m    268\u001B[39m             \u001B[38;5;28;01mwith\u001B[39;00m torch.cuda.device(device):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.virtualenvs/annalyticai/lib/python3.11/site-packages/torch/serialization.py:250\u001B[39m, in \u001B[36mvalidate_cuda_device\u001B[39m\u001B[34m(location)\u001B[39m\n\u001B[32m    247\u001B[39m device = torch.cuda._utils._get_device_index(location, \u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m    249\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch.cuda.is_available():\n\u001B[32m--> \u001B[39m\u001B[32m250\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[33m'\u001B[39m\u001B[33mAttempting to deserialize object on a CUDA \u001B[39m\u001B[33m'\u001B[39m\n\u001B[32m    251\u001B[39m                        \u001B[33m'\u001B[39m\u001B[33mdevice but torch.cuda.is_available() is False. \u001B[39m\u001B[33m'\u001B[39m\n\u001B[32m    252\u001B[39m                        \u001B[33m'\u001B[39m\u001B[33mIf you are running on a CPU-only machine, \u001B[39m\u001B[33m'\u001B[39m\n\u001B[32m    253\u001B[39m                        \u001B[33m'\u001B[39m\u001B[33mplease use torch.load with map_location=torch.device(\u001B[39m\u001B[38;5;130;01m\\'\u001B[39;00m\u001B[33mcpu\u001B[39m\u001B[38;5;130;01m\\'\u001B[39;00m\u001B[33m) \u001B[39m\u001B[33m'\u001B[39m\n\u001B[32m    254\u001B[39m                        \u001B[33m'\u001B[39m\u001B[33mto map your storages to the CPU.\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m    255\u001B[39m device_count = torch.cuda.device_count()\n\u001B[32m    256\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m device >= device_count:\n",
      "\u001B[31mRuntimeError\u001B[39m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "df_dfu['predicted'] = predict_it(df_dfu.loc[0, 'job_title'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-27T06:18:38.425235Z",
     "start_time": "2025-08-27T06:18:38.071244Z"
    }
   },
   "id": "7d2e7af362ddc9fd"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9506382978723404\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          No       0.91      1.00      0.95       538\n",
      "         Yes       1.00      0.91      0.95       637\n",
      "\n",
      "    accuracy                           0.95      1175\n",
      "   macro avg       0.95      0.95      0.95      1175\n",
      "weighted avg       0.95      0.95      0.95      1175\n",
      "\n",
      "Job Title: Network Administrator => Predicted is_it: Yes\n",
      "Job Title: Marketing Specialist => Predicted is_it: No\n",
      "Job Title: Cloud Engineer => Predicted is_it: Yes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "df = pd.read_csv('/Users/apoltavets/anna-apps/annalyticai/careear-stats/data/job_titles_classified.csv')\n",
    "df = df[df['title'].notna()]\n",
    "X = df['title']  # Feature: job titles  \n",
    "y = df['is_it']      # Target: is_it column  \n",
    "\n",
    "# Split the data into training and testing sets  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert job titles into numerical features using CountVectorizer  \n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# Train a Random Forest Classifier  \n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train_vec, y_train)\n",
    "\n",
    "# Make predictions  \n",
    "y_pred = model.predict(X_test_vec)\n",
    "\n",
    "# Evaluate the model  \n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Example: Predicting for new job titles  \n",
    "new_job_titles = ['Network Administrator', 'Marketing Specialist', 'Cloud Engineer']\n",
    "new_job_titles_vec = vectorizer.transform(new_job_titles)\n",
    "predictions = model.predict(new_job_titles_vec)\n",
    "\n",
    "for title, prediction in zip(new_job_titles, predictions):\n",
    "    print(f\"Job Title: {title} => Predicted is_it: {prediction}\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-29T01:47:39.371083Z",
     "start_time": "2025-08-29T01:47:37.418968Z"
    }
   },
   "id": "ffe99b8d860eae6b"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "27d00410b2d5cd4c"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x8/ps_xddr95734lgrb6gqpr7g80000gp/T/ipykernel_51498/2913625382.py:13: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined_df = pd.concat(dataframes, ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": "(13956, 9)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Specify the folder containing the CSV files  \n",
    "folder_path = '/Users/apoltavets/anna-apps/annalyticai/careear-stats/output/job_posting_details/'\n",
    "csv_files = [file for file in os.listdir(folder_path) if file.endswith('.tsv')] \n",
    "dataframes = [] \n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    df = pd.read_csv(file_path, sep='\\t')\n",
    "    dataframes.append(df)\n",
    "    \n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "print(combined_df.shape)\n",
    "combined_df.drop_duplicates(subset=['linkedin_job_id'], inplace=True)\n",
    "print(combined_df.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-29T01:56:35.252584Z",
     "start_time": "2025-08-29T01:56:34.413869Z"
    }
   },
   "id": "d8846ce02cee018f"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x8/ps_xddr95734lgrb6gqpr7g80000gp/T/ipykernel_51498/4034364273.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  combined_df.drop_duplicates(subset=['organization_name', 'payer_name', 'job_title', 'job_description'], inplace=True)\n",
      "/var/folders/x8/ps_xddr95734lgrb6gqpr7g80000gp/T/ipykernel_51498/4034364273.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  combined_df['job_title_upd'] = combined_df['job_title'].apply(title_cleanup)\n"
     ]
    }
   ],
   "source": [
    "combined_df = combined_df[combined_df['job_title'].notna()]\n",
    "combined_df.drop_duplicates(subset=['organization_name', 'payer_name', 'job_title', 'job_description'], inplace=True)\n",
    "import re\n",
    "\n",
    "def title_cleanup(job_title):\n",
    "    if not isinstance(job_title, str):  # Check if the value is not a string\n",
    "        return job_title\n",
    "\n",
    "    tmp = re.sub(r'\\(.*?\\)', '', job_title).strip()\n",
    "    tmp1 = re.sub(r'-\\s*\\d+[,\\d]*/\\w+\\s+\\w+$', '', tmp).strip()\n",
    "    tmp2 = tmp1.replace('Sr.', 'Senior').replace('Sr ', 'Senior ')\n",
    "    return tmp2\n",
    "\n",
    "# Apply the function\n",
    "combined_df['job_title_upd'] = combined_df['job_title'].apply(title_cleanup)\n",
    "combined_df = combined_df[combined_df['job_title_upd'].notna()]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-29T02:17:17.057837Z",
     "start_time": "2025-08-29T02:17:16.998442Z"
    }
   },
   "id": "503906747eb99d18"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "(7661, 2883)"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "vectorizer = load(MODEL_PATH)\n",
    "model = load(ENCODER_PATH)\n",
    "\n",
    "xvec = vectorizer.transform(combined_df['job_title_upd'])\n",
    "xvec.shape\n",
    "combined_df['is_it'] = model.predict(xvec)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-29T02:17:44.335817Z",
     "start_time": "2025-08-29T02:17:44.242584Z"
    }
   },
   "id": "8ffd603bb1f98e97"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "(4325, 11)"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_it = combined_df[combined_df['is_it'] == 'Yes']\n",
    "df_it.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-29T02:20:23.894684Z",
     "start_time": "2025-08-29T02:20:23.881870Z"
    }
   },
   "id": "38f4b9cba4a7f01e"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "job_post_date\n2025-07-06    4063\n2025-07-07     262\nName: count, dtype: int64"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_it['job_post_date'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-29T02:21:36.084421Z",
     "start_time": "2025-08-29T02:21:36.049965Z"
    }
   },
   "id": "8c1228710f98b6fa"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/apoltavets/anna-apps/annalyticai/careear-stats/data/LinkedIn_Titles_Marked_20250829.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-30T02:22:24.072464Z",
     "start_time": "2025-08-30T02:22:23.920766Z"
    }
   },
   "id": "3e4ce492acb86e3e"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "                                          job_title_upd  \\\n0                                    IT Service Manager   \n1                   IT Business Systems Administrator I   \n2                                 Digital Print Manager   \n3                              Senior Software Engineer   \n4     Applied Integration and Power Systems Research...   \n...                                                 ...   \n4102      Java / Cloud Solutions Architect/ W2 contract   \n4103             Site Reliability Engineer /w2 contract   \n4104                          Java Full Stack Developer   \n4105                       Software Engineer, Inference   \n4106                                   Systems Engineer   \n\n                         organization_name   is_IT  \n0                    Modern Office Methods      IT  \n1                  GreenState Credit Union      IT  \n2                         A Hiring Company  Non-IT  \n3                                Enlighten      IT  \n4     National Renewable Energy Laboratory      IT  \n...                                    ...     ...  \n4102                             Dice - JW      IT  \n4103                             Dice - JW      IT  \n4104                             Dice - JW      IT  \n4105                             Anthropic      IT  \n4106                           Quevera LLC      IT  \n\n[4107 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>job_title_upd</th>\n      <th>organization_name</th>\n      <th>is_IT</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>IT Service Manager</td>\n      <td>Modern Office Methods</td>\n      <td>IT</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>IT Business Systems Administrator I</td>\n      <td>GreenState Credit Union</td>\n      <td>IT</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Digital Print Manager</td>\n      <td>A Hiring Company</td>\n      <td>Non-IT</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Senior Software Engineer</td>\n      <td>Enlighten</td>\n      <td>IT</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Applied Integration and Power Systems Research...</td>\n      <td>National Renewable Energy Laboratory</td>\n      <td>IT</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4102</th>\n      <td>Java / Cloud Solutions Architect/ W2 contract</td>\n      <td>Dice - JW</td>\n      <td>IT</td>\n    </tr>\n    <tr>\n      <th>4103</th>\n      <td>Site Reliability Engineer /w2 contract</td>\n      <td>Dice - JW</td>\n      <td>IT</td>\n    </tr>\n    <tr>\n      <th>4104</th>\n      <td>Java Full Stack Developer</td>\n      <td>Dice - JW</td>\n      <td>IT</td>\n    </tr>\n    <tr>\n      <th>4105</th>\n      <td>Software Engineer, Inference</td>\n      <td>Anthropic</td>\n      <td>IT</td>\n    </tr>\n    <tr>\n      <th>4106</th>\n      <td>Systems Engineer</td>\n      <td>Quevera LLC</td>\n      <td>IT</td>\n    </tr>\n  </tbody>\n</table>\n<p>4107 rows Ã— 3 columns</p>\n</div>"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-30T02:22:30.565700Z",
     "start_time": "2025-08-30T02:22:30.528627Z"
    }
   },
   "id": "4ebb3e0d0593efc8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "fa869f5a868f464b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
